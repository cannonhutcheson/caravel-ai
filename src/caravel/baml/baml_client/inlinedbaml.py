###############################################################################
#
#  Welcome to Baml! To use this generated code, please run the following:
#
#  $ pip install baml-py
#
###############################################################################

# This file was generated by BAML: please do not edit it. Instead, edit the
# BAML files and re-generate this code.
#
# ruff: noqa: E501,F401
# flake8: noqa: E501,F401
# pylint: disable=unused-import,line-too-long
# fmt: off

file_map = {
    
    "APIUtil.baml": "\ntemplate_string NoMarkdown() #\"\n    Do not use markdown.\n\"#\n\ntemplate_string NotEnoughContextError(error_message: string, required: string[]) #\"\n    NotEnoughContextError: {{ error_message }}, required: {{ required }}\n\"# \n\ntemplate_string EmptyDictString() #\"\n    {}\n\"#\n\ntemplate_string EmptyString() #\"\n    \n\"# \n\nclass DynamicJsonObject {\n    @@dynamic\n} // DynamicJsonObject\n\nenum DynamicJsonEnum {\n    @@dynamic\n} // DynamicJsonEnum\n\n\nfunction GenerateHumanLanguageResponse(json: string, context: string) -> string {\n    client \"openai/gpt-4o-mini\"\n    prompt #\"\n\n        Describe the content within the following\n        {{ json }}\n        as if you are answering the prompt in\n        {{ context }}\n        in a single paragraph. Do not include technical details, just describe the content itself.\n        \n        {{ ctx.output_format }}\n    \"#\n} // GenerateHumanLanguageResponse\n\nfunction DynamicGenerateReqBody(context: string) -> DynamicJsonObject {\n    client \"openai/gpt-4o-mini\"\n    prompt #\"\n        Fill out the request body based on the content of the string.\n        {{ ctx.output_format }}\n    \"#\n\n}\n\nenum HTTPMethod {\n    GET\n    DELETE\n    POST\n    PATCH\n    PUT\n}\n\nclass APIRequest {\n    path string @description(#\"\n        This is the path that will be attached to the base URL in order to make the API Request.\n    \"#)\n    \n    method HTTPMethod @description(#\"\n        The HTTP method to be employed by the API Request.\n    \"#)\n\n    params map<string, string> @description(#\"\n        The query parameters that will be used in the request.\n    \"#)\n    request_body string?\n} // APIRequest\n\n// map<string, string> works here because query paramas are ALWAYS strs\nfunction PopulateQueryParameters(fmt: map<string, string | int | float | bool>, context: string) -> map<string, string> {\n    client \"openai/gpt-4o-mini\"\n    prompt #\"\n        DO NOT HALLUCINATE QUERY PARAMS NOT IN {{ fmt }}\n\n        Using the following query parameter structure:\n        {{ fmt }}\n        And the provided context: {{ context }},\n        Complete the query parameters. If the query param structure is empty, return {{ fmt }} with appropriate default values populated.\n        {{ ctx.output_format }}\n    \"#\n} // newPopulateQP\n\n\n\ntest new_function {\n    functions [PopulateQueryParameters]\n    args {\n        fmt {\n            \"start_cursor\" \"\"\n            \"filter[acquisition_type][eq]\" \"loan | lease\"\n            \"filter[reference_number][like]\" \"\"\n            \"filter[vehicle_id][eq]\" \"\"\n            \"filter[lender_id][eq]\" \"\"\n            \"filter[vendor_id][eq]\" \"\"\n            \"filter[acquisition_date][lt]\" \"\"\n            \"filter[created_at][lt]\" \"\"\n            \"filter[updated_at][lt]\" \"\"\n            \"sort[id]\" \"desc\"\n            \"sort[acquisition_date]\" \"asc | desc\" \n            \"sort[created_at]\" \"asc | desc\"\n            \"sort[updated_at]\" \"asc | desc\"\n        }\n        context #\"\n            Get all of my loan acquisitions from 2010 to 2020 sorted in chronological order.\n        \"#\n    }\n}\n\n\n\n",
    "Dynamic.baml": "\n\n// A dynamic class that will be used for checking the schema of OpenAPI specifications\nclass DynamicObject {\n    @@dynamic\n} // DynamicObject\n\n// A dynamic enum that will be used for constructing dynamic json objects.\nenum DynamicEnum {\n    @@dynamic\n} // DynamicEnum\n\nfunction ExtractDynamicTypes(context: string) -> DynamicObject{\n    client \"openai/gpt-4o-mini\"\n    prompt #\"\n        {{ ctx.output_format }}\n        {{ _.role('user') }}\n        Extract from this content:\n        {{ context }}\n    \"#\n} // ExtractDynamicTypes\n\n\nclass DynamicAPIRequest {\n    path string\n    method HTTPMethod\n    params  map<string, string>?\n    request_body DynamicObject?\n} // DynamicAPIRequest\n\nfunction ConstructDynamicAPIRequest(path: string, method: HTTPMethod | string, params: map<string, string>?, request_body: DynamicObject?) -> DynamicAPIRequest {\n    client \"openai/gpt-4o-mini\"\n    prompt #\"\n        Using\n        {{ path }}\n        {{ method }}\n        {{ params }}\n        {{ request_body }}\n        {{ ctx.output_format }}\n    \"#\n} // ConstructDynamicAPIRequest\n\ntest api_req_dyn {\n    functions [ConstructDynamicAPIRequest]\n    args {\n        path \"/addresses\"\n        method GET\n    } // args\n}",
    "Intent.baml": "function GetIntent(intents: string[], intent: string) -> string @description(#\"\n    A string formatted in the following manner: <METHOD Intent>, where METHOD is the HTTP Request that will need to be used. \n\"#) {\n    client \"openai/gpt-4o-mini\"\n    prompt #\"\n        Using the following list of intents, \n        {{ intents }}\n        and the user-prompted intent:\n        {{ intent }}\n        output the intent, sans quotes.\n        {{ ctx.output_format }}   \n    \"#\n} // GetIntent\n\n\nfunction MapContextToParams(context: string) -> map<string, string> {\n    client \"openai/gpt-4o-mini\"\n    prompt #\"\n        Parametrize the context:\n        {{ context }}\n        Such that a mapping is created between the parameters given in the context and their values.\n\n        {{ ctx.output_format }}\n    \"#\n}\n\ntest map_ctx {\n    functions [MapContextToParams]\n    args {\n        context \"I want to add a new vehicle to fleetio. The name is cannons truck, the vehicles type id is 10, the unit metrics are miles, and the status id is 20. \"\n    }\n}\n\ntest get_intent {\n    functions [GetIntent]\n    args {\n        intents [\n            \"POST Add a new vehicle to Fleetio\",\n            \"GET Get a vehicle from Fleetio\",\n            \"PATCH Edit a vehicle in Fleetio\" \n        ]\n        intent #\"upload a new vehicle to fleetio\"#\n    } // args\n} // get_intent",
    "MakePath.baml": "\ntemplate_string NoParamProvided() #\"\n    NoParamProvidedError\n\"# // NoParamProbided\n\nfunction MakePath(path: string, user_input: string) -> string @description(#\"\n    A string representing the altered path, filled with the path parameters based on the user input.\n\"#) {\n    client \"openai/gpt-4o-mini\"\n    prompt #\"\n        Using this path outline:\n        {{ path }}\n        Generate the path with path parameters (if any exist) filled in, using information gleaned from this input:\n        {{ user_input }}\n        If the parameter cannot be inferred, return:\n        {{ NoParamProvided() }}\n         If no path parameter exists within {{ path }}, return only:\n        {{ path }}\n        {{ ctx.output_format }}\n    \"#\n} // MakePath\n\ntest make_path_param_given {\n    functions [MakePath]\n    args {\n        path #\"/v1/vehicles/{id}\"#\n        user_input #\"Get vehicle with id 1234\"#\n    }\n}\n\ntest make_path_param_not_given {\n    functions [MakePath]\n    args {\n        path #\"/v1/vehicles/{id}\"#\n        user_input #\"Can you get my dad's truck from fleetio? \"#\n    }\n}\n\ntest make_path_no_path_param_present {\n    functions [MakePath]\n    args {\n        path #\"/v1/vehicles/\"#\n        user_input #\"Can you give me all of my vehicles in fleetio?\"#\n    }\n}",
    "ParsingTransform.baml": "\nfunction CreateDescription(current_api_description: string, descriptions: string[]) -> string {\n    client \"openai/gpt-4o-mini\"\n    prompt #\"\n        Using the following description:\n        {{ current_api_description }}\n        Output a single-sentence description that makes sense in human language. Be sure the description you create does not already exist here:\n        {{ descriptions }}\n        {{ ctx.output_format}}\n    \"#\n} // CreateDescription\n\ntest create_description_test {\n    functions [CreateDescription]\n    args {\n        current_api_description #\"\"Cancels an Account that belongs to your organization.\\n\\n:::info\\nThis endpoint is only usable by Fleetio partners with an Organization Token or Partner Token.\\n:::\",\"#\n        descriptions []\n    }\n}",
    "clients.baml": "// Learn more about clients at https://docs.boundaryml.com/docs/snippets/clients/overview\n\nclient<llm> CustomGPT4o {\n  provider openai\n  options {\n    model \"gpt-4o\"\n    api_key env.OPENAI_API_KEY\n  }\n}\n\nclient<llm> CustomGPT4oMini {\n  provider openai\n  retry_policy Exponential\n  options {\n    model \"gpt-4o-mini\"\n    api_key env.OPENAI_API_KEY\n  }\n}\n\nclient<llm> CustomSonnet {\n  provider anthropic\n  options {\n    model \"claude-3-5-sonnet-20241022\"\n    api_key env.ANTHROPIC_API_KEY\n  }\n}\n\n\nclient<llm> CustomHaiku {\n  provider anthropic\n  retry_policy Constant\n  options {\n    model \"claude-3-haiku-20240307\"\n    api_key env.ANTHROPIC_API_KEY\n  }\n}\n\n// https://docs.boundaryml.com/docs/snippets/clients/round-robin\nclient<llm> CustomFast {\n  provider round-robin\n  options {\n    // This will alternate between the two clients\n    strategy [CustomGPT4oMini, CustomHaiku]\n  }\n}\n\n// https://docs.boundaryml.com/docs/snippets/clients/fallback\nclient<llm> OpenaiFallback {\n  provider fallback\n  options {\n    // This will try the clients in order until one succeeds\n    strategy [CustomGPT4oMini, CustomGPT4oMini]\n  }\n}\n\n// https://docs.boundaryml.com/docs/snippets/clients/retry\nretry_policy Constant {\n  max_retries 3\n  // Strategy is optional\n  strategy {\n    type constant_delay\n    delay_ms 200\n  }\n}\n\nretry_policy Exponential {\n  max_retries 2\n  // Strategy is optional\n  strategy {\n    type exponential_backoff\n    delay_ms 300\n    mutliplier 1.5\n    max_delay_ms 10000\n  }\n}",
    "generators.baml": "// This helps use auto generate libraries you can use in the language of\n// your choice. You can have multiple generators if you use multiple languages.\n// Just ensure that the output_dir is different for each generator.\ngenerator target {\n    // Valid values: \"python/pydantic\", \"typescript\", \"ruby/sorbet\", \"rest/openapi\"\n    output_type \"python/pydantic\"\n\n    // Where the generated code will be saved (relative to baml_src/)\n    output_dir \"../\"\n\n    // The version of the BAML package you have installed (e.g. same version as your baml-py or @boundaryml/baml).\n    // The BAML VSCode extension version should also match this version.\n    version \"0.77.2\"\n\n    // Valid values: \"sync\", \"async\"\n    // This controls what `b.FunctionName()` will be (sync or async).\n    default_client_mode async\n}\n",
    "resume.baml": "// Defining a data model.\nclass Resume {\n  name string\n  email string\n  experience string[]\n  skills string[]\n}\n\n// Create a function to extract the resume from a string.\nfunction ExtractResume(resume: string) -> Resume {\n  // Specify a client as provider/model-name\n  // you can use custom LLM params with a custom client name from clients.baml like \"client CustomHaiku\"\n  client \"openai/gpt-4o\" // Set OPENAI_API_KEY to use this client.\n  prompt #\"\n    Extract from this content:\n    {{ resume }}\n\n    {{ ctx.output_format }}\n  \"#\n}\n\n// Test the function with a sample resume. Open the VSCode playground to run this.\ntest vaibhav_resume {\n  functions [ExtractResume]\n  args {\n    resume #\"\n      Vaibhav Gupta\n      vbv@boundaryml.com\n\n      Experience:\n      - Founder at BoundaryML\n      - CV Engineer at Google\n      - CV Engineer at Microsoft\n\n      Skills:\n      - Rust\n      - C++\n    \"#\n  }\n}\n",
}

def get_baml_files():
    return file_map